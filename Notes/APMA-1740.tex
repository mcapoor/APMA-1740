\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{letterpaper, margin=0.25in}
\usepackage{graphicx} 
\usepackage{parskip}
\usepackage{booktabs}
\usepackage{array} 
\usepackage{paralist} 
\usepackage{verbatim}
\usepackage{subfig}
\usepackage{fancyhdr}
\usepackage{sectsty}
\usepackage[shortlabels]{enumitem}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt} 
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} 
\usepackage[titles,subfigure]{tocloft}
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} %

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{empheq}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{tikz-cd}
\usepackage{tikz-qtree}
\pgfplotsset{compat=1.18}
\usetikzlibrary{calc, intersections, decorations.markings}

\usetikzlibrary{external}
\tikzexternalize
\tikzsetexternalprefix{figures/}

\newcommand{\ans}[1]{\boxed{\text{#1}}}
\newcommand{\vecs}[1]{\langle #1\rangle}
\renewcommand{\hat}[1]{\widehat{#1}}

\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}

\newcommand{\ind}{\mathbbm{1}}
\newcommand{\qed}{\quad \blacksquare}

\newcommand{\brak}[1]{\left\langle #1 \right\rangle}
\newcommand{\bra}[1]{\left\langle #1 \right\vert}
\newcommand{\ket}[1]{\left\vert #1 \right\rangle}

\newcommand{\abs}[1]{\left\vert #1 \right\vert}
\newcommand{\norm}[1]{\left\vert\left\vert #1 \right\vert\right\vert}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}

\newcommand{\Var}{\text{Var}\,}
\newcommand{\Cov}{\text{Cov}\,}

\newcommand{\mfX}{\mathfrak{X}}
\newcommand{\ep}{\varepsilon}

\newcommand{\Ec}{\mathcal{E}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\X}{\chi}
\renewcommand{\L}{\mathcal{L}}

\newcommand{\sub}{\subseteq}
\newcommand{\st}{\text{ s.t. }}
\newcommand{\card}{\text{card }}
\renewcommand{\div}{\vspace*{10pt}\hrule\vspace*{10pt}}
\newcommand{\surj}{\twoheadrightarrow}
\newcommand{\inj}{\hookrightarrow}
\newcommand{\biject}{\hookrightarrow \hspace{-8pt} \rightarrow}
\renewcommand{\bar}[1]{\overline{#1}}
\newcommand{\overcirc}[1]{\overset{\circ}{#1}}
\newcommand{\diam}{\text{diam }}

\renewcommand{\Re}{\text{Re}\,}
\renewcommand{\Im}{\text{Im}\,}
\newcommand{\sign}{\text{sign}\,}

\newcommand{\iid}{\overset{\text{iid}}{\sim}}
\newcommand*{\tbf}[1]{\ifmmode\mathbf{#1}\else\textbf{#1}\fi}

\newcommand{\MSE}{\text{MSE}\,}
\newcommand{\ISE}{\text{ISE}\,}


\DeclareMathOperator*{\argmax}{\arg\max}
\DeclareMathOperator*{\argmin}{\arg\min}


\usepackage{tcolorbox}
\tcbuselibrary{breakable, skins}
\tcbset{enhanced}
\tcbset{shield externalize}

\newenvironment*{tbox}[2][gray]{
    \begin{tcolorbox}[
        parbox=false,
        colback=#1!5!white,
        colframe=#1!75!black,
        breakable,
        title={#2}
    ]}
    {\end{tcolorbox}}

\newenvironment*{exercise}[1][red]{
    \begin{tcolorbox}[
        parbox=false,
        colback=#1!5!white,
        colframe=#1!75!black,
        breakable
    ]}
    {\end{tcolorbox}}

\newenvironment*{proof}[1][blue]{
\begin{tcolorbox}[
    parbox=false,
    colback=#1!5!white,
    colframe=#1!75!black,
    breakable
]}
{\end{tcolorbox}}

\newenvironment*{proposition}[1][gray]{
\begin{tcolorbox}[
    parbox=false,
    colback=#1!5!white,
    colframe=#1!75!black,
    breakable
]}
{\end{tcolorbox}}

\title{APMA 1740: Recent Applications of Probability and Statistics}
\author{Milan Capoor}
\date{Spring 2025}

\begin{document}
\maketitle

\chapter{Information Theory}
\section{Jan 22}
\subsection{Maximum Entropy Principle}
\tbf{A strange though experiment of Gibbs:} Imagine a physical system $S$ (say a gas) in an ``infinite bath''. Let $x$ be the state of every particle (positions, velocities, ...) in $S$.

For simplicity, let $S$ be be $3$ particles in $\Z^2$ with $x \in \Z^6$ being the positions. Let $s$ be the number of states of particles in $S$.

\emph{What is $p(x)$, the probability that $S$ has state $x$?}

In the simplest case (each particle is independent and the state distribution is uniform), we trivially have $P(x) = \frac{1}{s}$. But in general, these are incredibly strong assumptions.

We can create some constraints to do better.

\begin{enumerate}
    \item Assume that the average kinetic energy $\Ec$ of the infinite heat bath is some constant $\theta$.

          In this case, we expect the average kinetic energy of $S$ is approximately $\theta$:
          \[\sum_x  p(x) \Ec(x) = \theta\]

    \item Trivially, $p$ is a probability distribution, so
          \[\sum_x p(x) = 1\]
\end{enumerate}

But still this is far from enough: this gives us only $2$ constraints for $s$ many unknowns!

However, we can approximate with the LLN. Sample $n \gg s \gg 1$ iid copies of $S$, $S_1, S_2, \dots, S_n$ with positions $x_1, x_2, \dots, x_n$.

Define the \tbf{empirical distribution}
\[\hat p_x = \frac{\#\{i: X_i = x\}}{n}\]

So with large $n$, $\hat p = p$, and
\[\sum_x \hat p(x) \Ec(x) \approx \theta\]

\emph{Claim:} The vast majority of assignments of states to $X_1, \dots, X_n$ yield a single empirical distribution $\hat p$.

Consider $C(\hat p)$, the number of ways to assign a state to each of $n$ systems that would yield $\hat p$. Then, with $\hat n_x = \hat p_x \cdot n = \#\{i: X_i = x\}$,
\[\textcolor{red}{C(\hat p) = \binom{n}{\prod_{i=1}^s n_i}}\]

\section{Jan 24}
\tbf{Recall:} For a system $S$ with $s$ states, what is the probability $p(x)$ that $S$ is in state $x$?

We know that $\sum_{x=1}^s p(x) = 1$ and $\sum_{x=1}^s p(x) \Ec(x) = \theta$ for some constant $\theta$.

We sample $X_1, \dots, X_n$ iid from $S$ ($n \gg s\gg 1$) and define the empirical distribution $\hat p_x = \frac{\#\{i: X_i = x\}}{n}$. By LLN, $\hat p \approx p$.

\tbf{Claim:} $\hat p$ should maximize $C(\hat p)$, the number of arrangements of $n$ states $\{1, \dots, s\}$ that yield $\hat p$:
\[C(\hat p) = \binom{n}{\hat p_1 n\dots \hat p_s n} = \frac{n!}{(\hat p_1 n)! \dots (\hat p_s n)!}\]
where $\hat p_i n$ is the number of times we see state $i$ in the sample.

\emph{Example:} For $s = 2$, put $n$ balls into $2$ bins $\{1, 2\}$. Then $\hat p_1 n = a$ balls in bin 1, $\hat p+2 n = n- a$ balls in bin 2. We write this
\[C(\hat p) = \binom{n}{a} = \binom{n}{a, n-a} = \frac{n!}{a!(n-a)!}\]

\tbf{Stirling's Approximation:}
\[k! \approx \frac{k^k}{e^k} \sqrt{2\pi k}\]

Hence,
\begin{align*}
    C(\hat p)                  & = \frac{n^n e^{-n} \sqrt{2\pi n}}{\prod_{i=1}^s (\hat p_i n)^{\hat p_i n} e^{-\hat p_i n} \sqrt{2\pi \hat p_i n}}                           \\
    \log C(\hat p)             & = n\log n - n + \log\sqrt{2\pi n} -\sum_{i=1}^s \left[\hat p_i n \log (\hat p_i n) - \hat p_i n + \log \sqrt{2\pi n}\right]                 \\
    \frac{1}{n} \log C(\hat p) & = \log n - 1 + \frac{1}{n}\log\sqrt{2\pi n}-\sum_{i=1}^s \left[\hat p_i \log (\hat p_i n) - \hat p_i + \frac{1}{n}\log \sqrt{2\pi n}\right] \\
                               & = \log n - \frac{1}{n}\log\sqrt{2\pi n}-\sum_{i=1}^s \left[\hat p_i \log (\hat p_i) + \frac{1}{n}\log \sqrt{2\pi n}\right]                  \\
                               & = -\sum_{i=1}^s \hat p_i \log \hat p_i - \frac{1}{n} \sum_{i=1}^s \log \sqrt{2\pi \hat p_i n} + \frac{1}{n}\log \sqrt{2\pi n}
\end{align*}

Since, $\hat p_i \leq 1$, $\frac{1}{n} \log \sqrt{2\pi \hat p_i n} \leq \log n$. Further, $\frac{\log n}{n} \to 0$ so
\[\frac{1}{n} \log C(\hat p) \approx -\sum \hat p_i \log \hat p_i\]

\tbf{Definition:} If $p$ is a probability distribution, its \tbf{Shannon Entropy} is
\[H(p) = \sum p(x) \log \frac{1}{p(x)} = -\sum p(x) \log p(x)\]

\emph{Note:} $H(p) \geq 0$ since $p(x) \leq 1$ for all $p$.

Back to our original problem, we seek $\hat p$ that satisfies
\begin{itemize}
    \item $\sum_{x=1}^s \hat p_x = 1$
    \item $\sum_{x=1}^s \hat p_x \Ec(x) \approx \theta$
    \item $\hat p$ maximizes $C(\hat p)$, i.e. maximizes Shannon Entropy $H(\hat p)$
\end{itemize}

We turn to our trusty friend, Lagrange multipliers. We seek to chose $p$ to maximize
\[H(p) + \gamma \sum_{x=1}^s p_x + \lambda \sum_{x=1}^s p_x \Ec(x)\]

Taking derivatives WRT $p_x$,
\begin{align*}
    \frac{\partial}{\partial p_x} \left[H(p) + \gamma \sum_{x=1}^s p_x + \lambda \sum_{x=1}^s p_x \Ec(x)\right] & = \frac{\partial}{\partial p_x} \left[-\sum_x p_x \log p_x\right] +\gamma + \lambda \Ec(x) \\
                                                                                                                & = -\log p_x - 1 + \gamma + \lambda \Ec(x) = 0
\end{align*}

So $\gamma + \lambda \Ec(x) - 1 = \log p(x)$ and
\begin{align*}
    p(x) & = e^{-1} e^{\lambda \Ec(x)} e^{\gamma + \lambda \Ec(x)} \\
         & = \frac{1}{z_{\lambda}} e^{\lambda \Ec(x)}
\end{align*}
where $Z_{\lambda} = \sum_{x=1}^s e^{\lambda \Ec(x)}$.

To find $\lambda$, we use the constraint $\sum p_{x} \Ec(x) \theta$.

\section{Jan 27}
\tbf{Example:} Find the maximum entropy distribution $p$ on $\{1, 2, 3\}$ (i.e. $s = 3$) satisfying $\E_p X^2 = 2$, i.e. $\sum_{x=1}^s p_x x^2 = 2$.

Since $\E_p X^2 = \sum_{x=1}^s p(x) x^2 = 2$, $\Ec(x) = x^2$,
\[p(x) = \frac{1}{Z} e^{\lambda \Ec(x)} = \frac{1}{Z} e^{\lambda x^2}, \quad x = 1, 2, 3\]

We need to find $Z, \lambda$ satisfying
\begin{itemize}
    \item $\E_p X^2 = 2$
    \item $\sum p_x = 1$
\end{itemize}

Hence,
\begin{align*}
    \begin{cases}
        \frac{1}{Z}[e^{\lambda} + 4e^{4\lambda} + 9e^{9\lambda}] = 2 \\
        \frac{1}{Z}[e^{\lambda} + e^{4\lambda} + e^{9\lambda}] = 1
    \end{cases} & \implies Z = e^{\lambda} + e^{4\lambda} + e^{9\lambda}                                                                                                         \\
                                                                                                                       & \implies e^{\lambda} + 4e^{4\lambda} + 9e^{9\lambda} = 2(e^{\lambda} + e^{4\lambda} + e^{9\lambda}) \\
                                                                                                                       & \implies e^{\lambda} - 2e^{4\lambda} - 7e^{9\lambda} = 0
\end{align*}

We can solve for $\lambda$ with any numeric method.


\subsection{Maximum Entropy Principle in the Continuum}
\tbf{Definition:} Let $p$ be a PDF. Its \tbf{entropy} is defined as
\[H(p) = -\int_{-\infty}^{\infty} p(x) \log p(x)\; dx\]

\tbf{Example (MEP with multiple constraints):} Find $p$ that maximizes $H(p)$ subject to
\[\begin{cases}
        \sum p_x \Ec_1(x) = \theta_1 \\
        \vdots                       \\
        \sum p_x \Ec_k(x) = \theta_k \\
        \sum p_x = 1
    \end{cases}\]

Our Lagrange multipliers are given by
\[\max\left[H(p) + \lambda_1 \sum p_x \Ec_1(x) + \lambda_2 \sum p_x \Ec_2(x) + \dots + \lambda_k \sum p_x \Ec_k(x) + \gamma \sum p_x\right]\]

Taking derivatives WRT $p_x$, we get
\begin{align*}
    H(p) & = -\log p_x - 1 + \lambda_1 \Ec_1(x) + \dots + \lambda_k \Ec_k(x) + \gamma = 0              \\
         & \implies p_x = \frac{1}{Z} \exp\left[\lambda_1 \Ec_1(x) + \dots + \lambda_k \Ec_k(x)\right]
\end{align*}

The rest follows as before.

\tbf{Example:} Find the max entropy density subject to $\E_p X^2 = 1$ and $\E_p X = 0$.

In this case,
\[p_x = \frac{1}{Z} \exp\left[\lambda_1 \Ec_1(x) + \lambda_2 \Ec_2(x)\right]\]
where
\[\Ec_1(x) = x^2, \quad \Ec_2(x) = x\]

Hence, we have constraints
\[\begin{cases}
        \frac{1}{Z} \left[\int_{-\infty}^{\infty} e^{\lambda_1 x^2 + \lambda_2 x} x^2\; dx\right] = 1 \\
        \frac{1}{Z} \left[\int_{-\infty}^{\infty} e^{\lambda_1 x^2 + \lambda_2 x} x\; dx\right] = 0   \\
        \frac{1}{Z} \left[\int_{-\infty}^{\infty} e^{\lambda_1 x^2 + \lambda_2 x}\; dx\right] = 1
    \end{cases}\]

We can complete the square to get the integrals in the forms of a Gaussian:
\[\frac{1}{Z}e^{\lambda_1 x^2  +\lambda_2 x} = \frac{1}{Z}\exp\left[\lambda_1 \left(x - \frac{\lambda_2}{2\lambda_2}\right)^2 \right] \sim N(\frac{\lambda_2}{2\lambda_1}, \frac{-1}{2\lambda_1})\]

But we have mean $0$ and variance $1$ so
\[\frac{\lambda_2}{2\lambda_1} = 0 \implies \lambda_2 = 0, \quad -\frac{1}{2\lambda_1} = 1 \implies \lambda_1 = -\frac{1}{2}\]

$Z$ follows from simply computing
\[Z = \int_{-\infty}^{\infty} \exp(\lambda_1 x^2 + \lambda_2 x)\; dx\]

\subsection{Large Deviation Principle}

\begin{tbox}{\textbf{Large Deviation Principle:} Take $p$ on $\{1, 2, \dots, s\}$, $\Ec: \{1, \dots, s\} \to \R$. Observe $X_1, X_2, \, \dots,\, X_n \overset{\text{iid}}{\sim} p$. Define
        \[\frac{1}{n}\sum_{x=1}^n \Ec(X_k) = \theta\]. Define the empirical distribution $\hat p_x = \frac{1}{n}\cdot \#\{i: X_i = x\}$. Then $\E_{\hat p}\, \Ec(X) = \theta$}

    \emph{Proof:}
    \begin{align*}
        \E_{\hat p}\, \Ec(X) & = \sum_{x=1}^s \hat p_x \Ec(x)                                   \\
                             & = \frac{1}{n}\sum_{x=1}^s \Ec(x) \sum_{i=1}^n \ind_{X_i}         \\
                             & = \frac{1}{n}\sum_{i=1}^n \sum_{x=1}^s \ind_{X_i=x} \cdot \Ec(x) \\
                             & = \frac{1}{n} \sum_{i=1}^{n} \Ec(X_i) = \theta
    \end{align*}
\end{tbox}

Let $q$ be some probability distribution on $\{1, \dots, s\}$. What is $\P(\hat p = q)$?

Recall that the $C(\hat p)$ function gave the number of ways to assign a state to each of $n$ systems that would yield $\hat p$. Similarly, here we have
\[\P(\hat p = q) = \binom{n}{n_1 \cdots n_s} \prod_{x=1}^s p_x^{q_x \cdot n}\]

\tbf{Example:} Take $X_1, X_2 \sim p$. Let $q = \frac{1}{2} \delta_\{1\} + \frac{1}{2} \delta_\{2\}$. What is $\P(\hat p = q)$?
\begin{enumerate}
    \item How many ways can we sample 5 and 1 from $X_1, X_2$? Two ways: $(1, 5)$ or $(5, 1)$.
    \item Now wat is the probability $X_1 = 1, X_2 = 5$? This is $p_1 p_5$. Similarly, $\P(X_1 = 5, X_2 = 1) = p_5 p_1$.
\end{enumerate}

Hence, $\P(\hat p = q) = 2p_1 p_5$.

\section{Jan 29}
\subsection{Relative Entropy Function}
\tbf{Motivation:}
\begin{itemize}
    \item $p$ a PMF $\{1, \dots, s\}$
    \item $\Ec: \{1, \dots, s\} \to \R$ an energy function
    \item $X_1, X_2, \, \dots,\, X_n \iid p$
    \item $\hat p$ the empirical distribution, $\hat p_x = \frac{1}{n}\cdot \#\{i: X_i = x\}$
\end{itemize}

\emph{Question:} what does $\hat p$ look like?

Let $q$ be a given PMF on $\{1, \dots, s\}$.

\tbf{Heuristic:} $\frac{1}{n} \log \P(\hat p = q) \approx -D(q \parallel p)$

\tbf{Remark:} We have to be careful about this approximation. Indeed, it holds under LLN for $q = p$ and since we can approximate $p$ via an arbitrary distribution, it holds in general under certain conditions. However, we could easily construct a pathological example:
\begin{itemize}
    \item $p = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$
    \item $q = (\frac{1}{3} + \frac{\sqrt{2}}{K}, \frac{1}{3} + \frac{\sqrt{2}}{K}, \frac{1}{3} + \frac{\sqrt{2}}{K})$ for very large $K$
\end{itemize}

Now since $p$ is rational, $\P(\hat p q) = 0$ so $\frac{1}{n}\log \P(\hat p = q) = -\infty$.

\tbf{KL Entropy:}
\[D(q \parallel p) = \sum_{x=1}^s q_x \log \frac{q_x}{p_x}\]
measures how close $q$ is to $p$.

\begin{tbox}{\textbf{Jensen's Inequality:} For every $g: \R \to \R$ convex,
        \[\E g(X) \geq g(\E X)\]}
    \emph{Special Case:} $\E(X^2) \geq (\E X)^2$

    \div

    \emph{Proof:} Consider the tangent line to $g$ at $c = \E X$: $y = g'(c) (x - c) + g(c)$.

    By convexity, $g(x) \geq g(c) + g'(c)(x-c)$ for all $x$.

    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[
                    axis lines = middle,
                    xlabel = $x$,
                    ylabel = $y$,
                    xtick=\empty,
                    xmin = -1, xmax = 5,
                    ymin = -1, ymax = 5,
                    hide y axis
                ]
                \addplot [
                    domain=-1:5,
                    samples=100,
                    color=red,
                ]
                {x^2};
                \addplot [
                    domain=0:5,
                    samples=100,
                    color=blue,
                ]
                {2*x - 1};

                \coordinate (A) at (1, 1);
            \end{axis}

            \draw[fill] (A) circle (0.1) node[below right] {$c$};
        \end{tikzpicture}
    \end{center}

    Hence,
    \[\E g(X) \geq \E g'(c) (X - c) + \E g(c) = g'(c) (\E X - c) + g(c) = g(c) = g(\E X)\]
\end{tbox}

\begin{tbox}{\textbf{Properties of KL Entropy:}
        \begin{enumerate}
            \item $D(q \parallel p) \geq 0$
            \item $D(q \parallel p) = 0 \iff q = p$
        \end{enumerate}}
    \emph{Proof:}
    \begin{enumerate}
        \item \begin{align*}
                  D(q \parallel p) & = \sum_{x=1}^s q_x \log \frac{q_x}{p_x} \\
                                   & = \E_q \log \frac{q(X)}{p(X)}           \\
                                   & = -\E_q \log \frac{p(X)}{q(X)}          \\
                                   & = -\E_q \log Y
              \end{align*}
              where $Y = \frac{p_x}{q_x}$. Define $g(y) = -\log y$.

              Note $g$ is convex: $g''(y) = \frac{1}{y^2} > 0$. Hence, by Jensen's inequality,
              \begin{align*}
                  \E g(Y) & \geq g(\E Y) = -\log (\E Y) = -\log\left(\E_q \frac{p_x}{q_x}\right) = -\log\underbrace{\left(\sum_{x=1}^s q_x \frac{p_x}{q_x}\right)}_{\sum p_x \leq 1}\geq 0
              \end{align*}

        \item For $Y = \frac{p_x}{q_x}$,
              \[\E Y = \sum q_x \frac{p_x}{q_x} = 1 \implies Y = \E Y \text{ a.s.} \implies \frac{p_x}{q_x} = 1 \text{ a.s.} \implies p_x = q_x \quad \forall x\text{ a.s.}\]
    \end{enumerate}
\end{tbox}

\tbf{Another Heuristic:}
\[\frac{1}{n} \log \P(\hat q = q) \approx -D(q \parallel p) = -\sum q_x \log \frac{q_x}{p_x}\]

Find
\[q = \argmax_{\sum q_x \Ec(x) = \theta} (-D(q \parallel p))\]
using Lagrange multipliers

\section{Jan 31}
\tbf{Recall:} $D(q \parallel p) = 0$ iff $p = q$.

\begin{proof}
    \emph{Proof:}
    \begin{align*}
        D(q \parallel p) & = \sum_{x=1}^s q_x \log \frac{p_x}{q_x}                 \\
        X \sim q         & = \E[\log \frac{q_x}{p_x}] = -\E[\log \frac{p_x}{q_x}]  \\
                         & \overset{\text{Jensen}}{\geq} -\log[\E \frac{p_x}{q_x}] \\
                         & = -\log [\sum q_x \frac{p_x}{q_x}] = 0
    \end{align*}

    Hence, we get the equality iff $\E g(Y) = g(\E Y)$ where $Y = \frac{p_x}{q_x}$ ($x \sim q$) and $g(Y) = -\log Y$. ($g$ is strictly convex, i.e. $\E g(Y) = g(\E Y)$, iff $Y$ is a const a.s.)

    But since $Y = \E Y = 1$, $\frac{p_x}{q_x} = 1 \implies p_x = q_x$ a.s.
\end{proof}

Last time, we discussed the cases in which the approximation $\P(\hat p = q) \approx D(q \parallel p)$ fails. But why does this happen?

Recall
\[\P(\hat p = q) = \binom{n}{n_1\cdots n_s} \prod_i p_i^{n_i}\]
where $n_i = q_i \cdot n$.

But this binomial coefficient is well defined only if $q_i n \in \N$ for all $i$. Hence, the approximation only holds for distributions $q$ with $q_i \cdot n \in \N$ for all $i$.

\subsection{Sanov's Theorem}

\tbf{Motivation:} As usual, let $p$ be a PMF on $\{1, \dots, s\}$ and $X_1, X_2, \, \dots,\, X_n \iid p$. We know that for large $n$, $\hat p \approx p$. But this relation is only probabilistic. How do we quantify the probability that $\hat p$ is far from $p$?

\tbf{Example:} Let $s = 3$ and say $\hat p = (\hat p_1, \hat p_2, \hat p_3) = (a, b, c)$. Then
\[\begin{cases}
        a, b, c \geq 0 \\
        a + b + c = 1
    \end{cases}\]
gives us a triangle in $\R^3$:
\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
                axis lines = middle,
                xlabel = $x$,
                ylabel = $y$,
                zlabel = $z$,
                xtick=\empty,
                ytick=\empty,
                ztick=\empty,
                xmin = 0, xmax = 2,
                ymin = 0, ymax = 2,
                zmin = 0, zmax = 2,
                view={120}{45}
            ]
            \addplot3[
                patch,
                patch type=triangle,
                opacity=0.5,
                fill opacity=0.3,
                color=blue,
            ] coordinates {
                    (1, 0, 0)
                    (0, 1, 0)
                    (0, 0, 1)
                };

            % Draw a patch B on the surface 
            \node[red, scale=0.75] at (axis cs: 0.5, 0.25, 0.25) [below right] {$B$};
            \draw[red] (axis cs: 0.5, 0.25, 0.25) ellipse [x radius=0.2, y radius=0.2, rotate around={45:(axis cs: 0.5, 0.25, 0.25)}];

            % Plot random point p outside B 
            \draw[red, fill] (axis cs: 0.25, 0.5, 0.5) circle (0.01) node[below right, scale=0.75] {$p$};

            % Plot distinct point q 
            \draw[red, fill] (axis cs: 0.1, 0.04, 0.1) circle (0.02) node[above left, scale=0.75] {$p^*$};
        \end{axis}

    \end{tikzpicture}
\end{center}

\begin{tbox}{\textbf{Sanov's Theorem:} Let $B$ be an open subset of the space of all PMF on $\{1, \dots, s\}$. Then
        \[\lim_{n \to \infty} \frac{1}{n}\log \P(\hat p \in B) = -\inf_{q \in B} D(q \parallel p)\]
        Further, if $p^* = \argmin_{q \in B} D(q \parallel p)$ is unique, then
        \[\lim_{n \to \infty} \P(\norm{\hat p - p^*} > \ep \; | \; \hat p \in B) = 0 \quad \forall \ep > 0\]
        where $\norm{\hat p - p^*}$ is any metric, say $\norm{\hat p - p^*} = \max_{x \in \{1, \dots, s\}} \abs{\hat p_x - p_x}$}
    \emph{Proof:}
\end{tbox}

\tbf{Remark:} What if $p \in B$? Then $\inf_{q \in B} D(q \parallel p) = 0$, so
\[\frac{1}{n} \log \underbrace{e^{-o(n)}}\P(\hat p \in B) = 0\]


\section{Feb 5}
\tbf{Recall (Sanov's Theorem):} For $B$ open,
\begin{enumerate}
    \item \[\lim_{n \to \infty} \frac{1}{n} \log \P(\hat p_{x_1, \dots, x_n} \in B) = -\inf_{q\in B} D(q \parallel p)\]
    \item If $\exists ! \; p^* = \argmin_{q \in \bar B} D(q \parallel p)$, then
          \[\lim_{n \to \infty} \P(\norm{\hat p - p} > \ep \; | \; \hat p \in B) = 0 \quad \forall \ep >0 \]
\end{enumerate}


\begin{center}
    \begin{tikzpicture}
        % Draw a blob
        \fill[blue, opacity=0.3] plot[smooth cycle, tension=1.2] coordinates {
                (0, 0)
                (0.8, 0.3)
                (1.2, 1.1)
                (0.4, 2.5)
                (-1, 2.3)
                (-1.4, 1.6)
                (-1.2, 0.8)
                (-0.6, 0.2)
            };

        \draw[fill, red] (1.12, 1.5) circle (0.05) node[above right] {$p^*$};
        \draw[fill, red] (3, 2) circle (0.05) node[above right] {$p$};

        \node[blue, below right] at (0, 0) {$B$};
    \end{tikzpicture}
\end{center}

This leads to some interesting questions:
\begin{enumerate}
    \item Why is $p^*$ drawn on the boundary?
    \item Is there a case when $p^*$ lies in the interior?
\end{enumerate}

For the second: yes, if $p \in B$ (in which case $p$ is the global minimizer of $D(q \parallel p)$).

For the first, it suffices to show that since $D(q \parallel p)$ is a convex function, on any set $B$ with $p \notin B$, the minimizer $p^*$ must lie on the boundary.

\emph{Example:}

\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
                axis lines = middle,
                xlabel = $x$,
                ylabel = $y$,
                xtick=\empty,
                ytick=\empty,
                clip=false
            ]
            \addplot[blue, domain=-4:1] {x^2 + 1};
            \addplot[red, dashed, domain=1:2] {x^2 + 1};
            \addplot[blue, domain=2:4] {x^2 + 1};


            \addplot[red, ultra thick, dashed, domain=1:2] {0} node[below] {$B$};

            \coordinate (A) at (0, 1);

            \coordinate (B) at (1, 2);

        \end{axis}
        \draw[blue, fill] (A) circle (0.1) node[above left] {$p$};
        \draw[red] (B) circle (0.1) node[above] {$p^*$};
    \end{tikzpicture}
\end{center}

\emph{Example:} $B = \{q \; | \; \exists x: \abs{q_x - p_x} > 0\}$

By Sanov,
\[\P(\hat p_n \in B) \approx \exp(-n \inf_{q\in B} D(q \parallel p)) \leq e^{-n/2} < 10\%\]

Now let's prove the claim:
\begin{proof}
    \emph{Proof:}
    \begin{align*}
        F(q)                                              & = D(q \parallel p) = \sum q_x \log \frac{p_x}{q_x} \\
                                                          & = \sum q_x \log q_x - \sum q_x \log p_x            \\
        \frac{\partial F}{\partial q_x}                   & = \log q_x + 1 - \log p_x                          \\
        \frac{\partial^2 F}{\partial q_x \, \partial q_y} & = \begin{cases}
                                                                  1/q_x & x = y    \\
                                                                  0     & x \neq y
                                                              \end{cases}                                 \\
        H                                                 & = \begin{pmatrix}
                                                                  \frac{1}{q_1}                             \\
                                                                   & \frac{1}{q_2}                          \\
                                                                   &               & \ddots                 \\
                                                                   &               &        & \frac{1}{q_s}
                                                              \end{pmatrix}
    \end{align*}
    But $\forall v \in \R^s$, $v^T H v = \sum v_i^2 \frac{1}{q_i} \geq 0 \implies H$ is positive semi-definite. Hence $F$ is convex.
\end{proof}

\subsection{Back to Gibbs' Heat Bath}
Recall the original motivating example where $X_1, \dots, X_n \sim p$, and $\frac{1}{n} \sum_{i=1}^{n} \Ec(X_i) = \theta$.

Previously, we showed that $\theta = \frac{1}{n} \sum_{i=1}^{n} \Ec(X_i) = \E_{\hat p} [\Ec(X)]$.

Now consider the set $B = \{q \; | \; \E_q [\Ec(X)] > \theta\}$ and define $\Omega = \{q: \E_q [\Ec(X)] = \theta\}$.

\begin{center}
    \begin{tikzpicture}
        \fill[opacity=0.3] plot coordinates {
                (0, 0)
                (2, 2)
                (0, 2)
            };

        \draw[fill, red] (0.5, 1) circle (0.05) node[above] {$q$};
        \draw (0, 0) -- (2, 2) node[right] {$B$};

        \draw[fill, blue] (3, 0) circle (0.05) node[above] {$p$};
    \end{tikzpicture}
\end{center}
Imagine we observe some sample with energy higher than expected (i.e. $q \in B$). What is the probability of this occurring?

By Sanov, in order to find $\inf_{q \in B} D(q \parallel p)$, it suffices to find $p^*$ such that $D(p^* \parallel p) = \inf_{q \in B} D(q \parallel p)$.

In the past, we used Lagrange multipliers to confirm our solution is in the \tbf{exponential family}
\[p_x^* = \frac{1}{Z_{\lambda}} p_x \exp(\lambda \Ec(x)) \quad \forall x\]
for some $\lambda$.

\emph{Example of Exponential Family:} $\mathcal{N}(\mu, \sigma^2)$ has PDF $\frac{1}{Z} e^{-\frac{(x- \mu)^2}{2\sigma^2}}$

If instead we had many constraints $\E_{\hat p} [\Ec_i(X)] = \theta_i$ for $i = 1, \dots, k$, we found minimizer
\[p^* = \frac{1}{Z_{\lambda_1 \dots \lambda_k}} p_x \exp(\lambda_1 \Ec_1(x) + \dots + \lambda_k \Ec_k(x))\]
where we found $\lambda_1, \dots, \lambda_k$ using Lagrange multipliers to satisfy the constraints and
\[Z_{\lambda_1\dots \lambda_k} = \sum_x p_x \exp(\lambda_1 \Ec_1(x) + \lambda_k \Ec_k(x))\]

These must also satisfy:
\begin{enumerate}
    \item $\frac{\partial}{\partial \lambda_k} \log Z_k = \E_{\lambda}[\Ec_k(X)]$
    \item $\frac{\partial^2}{\partial \lambda_k \lambda_l} \log Z_k = \text{Cov}_{\lambda}(\Ec_k(X), \Ec_l(X)) \quad \forall k, l$
    \item $\log Z_k$ is a convex function of $\lambda$ and it is strictly convex unless $\exists \alpha = (\alpha_1, \dots, \alpha_k)$ such that $\alpha \neq 0$ and $\sum_{k=1}^c \alpha_k \Ec_k(x) = \text{const} \quad \forall x$
    \item $\log Z_{\lambda} - \sum \lambda_k \theta_k$ is convex in $\lambda$ and minimized when $\E_{\lambda}[\Ec(X)] =\theta_k$
\end{enumerate}

\section{Feb 7}
Last time, we defined the set
\[B = \{q: \E_q \Ec(X) < \theta\}\]

For $p \notin B$ known, we know that the minimizer $p^* = \argmin_{q\in B} D(q \parallel p)$ lies on the boundary of $B$, $\Omega = \{q: \E_q [\Ec(X)] = \theta\}$.

Using Lagrange Multipliers, we found
\[p_x^* = \frac{1}{Z_{\lambda}} p_x  e^{\lambda \Ec(x)} \quad \forall x\]
with
\[Z_{\lambda} = \sum_{x=1}^{s} p_xe^{\lambda \Ec(x)}\]

Now, we want to find $\lambda = (\lambda_1, \dots, \lambda_s)$ that satisfies
\[\E_{p^*}[\Ec(X)] = \theta \iff \sum p_x^* \Ec(x) = \theta \iff \sum \frac{1}{Z_{\lambda}} p_x e^{\lambda \Ec(x)} \Ec(x) = \theta\]

\begin{tbox}{\textbf{Proposition:}
        \begin{enumerate}
            \item $\frac{\partial}{\partial \lambda_k} \log Z_{\lambda} = \E_{\lambda}[\Ec_k(X)] \quad \forall k=1, \dots, c$
            \item $\frac{\partial^2}{\partial \lambda_k \, \partial \lambda_l} \log Z_{\lambda} = \Cov_{\lambda} (\Ec_k(X), \Ec_l(X)) \quad \forall k,l$
            \item $\log Z_{\lambda}$ is convex in $\lambda$ and, in general, strictly convex (unless the equations $\left\{\E_{p^*} \Ec_k(X) = \theta_k\right\}_{k=1}^c$ are redundant, i.e. $\not\exists b_1, \dots b_c \neq (0, \dots, 0)$)
            \item Assuming (3), the function
                  \[\log Z_{\lambda} - \sum_{k=1}^{c} \lambda_k \theta_k\]
                  is in general strictly convex and is minimized when
                  \[\E_{\lambda}[\Ec_k(X)] = \theta_k \quad \forall k\]
                  (i.e. at exactly the $\lambda$ that we need to find)
        \end{enumerate}
    }
    \emph{Proof:}

    \begin{enumerate}
        \item  \begin{align*}
                  \frac{\partial }{\partial \lambda_k} \log Z_{\lambda} & = \frac{1}{Z_k} \cdot \frac{\partial }{\partial \lambda_k} Z_{\lambda}                                                                       \\
                                                                        & = \frac{1}{Z_{\lambda}} \cdot \frac{\partial }{\partial \lambda_k} \left[\sum p_x e^{\lambda_1 \Ec_1(x) + \dots + \lambda_c \Ec_c(x)}\right] \\
                                                                        & = \frac{1}{Z_{\lambda}} \cdot \sum_x p_x e^{\lambda_1 \Ec_1(x) + \dots + \lambda_c \Ec_c(x)} \cdot \Ec_k(x)                                  \\
                                                                        & = \frac{1}{Z_{\lambda}}  \cdot \sum_x p_x \Ec_k(x) e^{\lambda \Ec(x)}                                                                        \\
                                                                        & = \sum_x p_x^* \Ec_k(x)                                                                                                                      \\
                                                                        & = \E_{p^*}[\Ec_k(X)] = \E_{\lambda}[\Ec_k(X)]
              \end{align*}

              \tbf{Remark:} We write $\E_{\lambda}$ instead of $\E_{p^*}$ just to emphasize that this is a function of $\lambda$

        \item

              \begin{exercise}
                  \textbf{Exercise:}  Email the proof to oanh\_nguyen1@brown.edu for bonus points.
              \end{exercise}

              \begin{proof}
                  \emph{Proof:}  In part 1, we showed that $\frac{\partial}{\partial \lambda_k} \log Z_{\lambda} = \E_{\lambda}[\Ec_k(X)]$. Hence, it suffices now to show
                  \[\frac{\partial}{\partial \lambda_l} \E_{\lambda}[\Ec_k(X)] = \Cov_{\lambda}(\Ec_k(X), \Ec_l(X))\]

                  \textcolor{red}{TODO}
              \end{proof}

        \item \[H(\lambda_1, \dots, \lambda_c) = \begin{pmatrix}
                      \frac{\partial^2}{\partial \lambda_k\, \partial \lambda_l} \log Z_{\lambda}
                  \end{pmatrix}_{c \times c}\]

              We need to show $\forall v \neq \vec 0$,
              \[v^T H v = \sum_{k, l}v_k v_l H_{kl} \geq 0 \implies \log_Z \text{ convex}\]

              But
              \begin{align*}
                  \sum v_k v_l H_{kl} & = \sum v_k v_l \Cov(\Ec_k(X), \Ec_l(X))     \\
                                      & = \Var\left(\sum v_k \Ec_k(X)\right) \geq 0
              \end{align*}
              since
              \[\sum v_k v_l \Cov(Y_k, T_l) = \Var\left(\sum v_k y_k\right)\]

    \end{enumerate}

\end{tbox}

\section{Feb 10}
Let $B = \{q: \E_q[\Ec(X)] < \theta\}$. Suppose we have two constraints
\begin{itemize}
    \item $\E_{\hat p} [\Ec_1(X)] = \theta_1$
    \item $\E_{\hat p} [\Ec_2(X)] = \theta_2$
\end{itemize}
and we know
\begin{itemize}
    \item $\E_p [\Ec_1(X)] > \theta_1$
    \item $\E_p [\Ec_2(X)] > \theta_2$
\end{itemize}

Then we can tighten
\[B = \{q: \E_q[\Ec_1(X]]< \theta_1, \; \E_q[\Ec_2(X)] > \theta_2\}\]
which updates our partition of the space from:

\begin{center}
    \begin{tikzpicture}
        \fill[opacity=0.3] plot coordinates {
                (0, 0)
                (2, 2)
                (0, 2)
            };

        \draw[fill, red] (0.5, 1) circle (0.05) node[above] {$q$};
        \draw (0, 0) -- (2, 2) node[right] {$B$};

        \draw[fill, blue] (2, 0) circle (0.05) node[above] {$p$};
    \end{tikzpicture}
    \hspace{1cm}
    \begin{tikzpicture}
        \node at (0, 0) {};
        \node at (0,1) {$\longrightarrow$};
    \end{tikzpicture}
    \hspace{1cm}
    \begin{tikzpicture}
        \fill[opacity=0.3] plot coordinates {
                (0, 0)
                (2, 2)
                (3, 0)
            };

        \draw[dashed] (0, 0) -- (2, 2) -- (3, 0) -- cycle;

        \node[left] at (1, 1) {$B$};
        \node[below] at (2, 0) {$\E_q[\Ec_2(X)] > \theta_2$};
        \node[right] at (3, 1) {$\E_q[\Ec_1(X)] < \theta_1$};

        \draw[fill, red] (3, 0) circle (0.1);
    \end{tikzpicture}
\end{center}
which tells us
\[\Omega = \{q: \E_q[\Ec_1(X)] = \theta_1, \quad \E_q[\Ec_2(X)] = \theta_2\} \]

We already know what to do if $p^* \in \Omega$, so consider just one constraint:
\[\E_q[\Ec_2(X)] = \theta_2\]

We can easily find $p_2^*$ WRT this constraint:
\begin{align*}
    B_2      & = \{q: \E_q[\Ec_2(X)] > \theta_2\}          \\
    \Omega_2 & = \{q: \E_q[\Ec_2(X)] = \theta_2\}
    p_2^*    & = \argmin_{q \in \Omega_2} D(q \parallel p)
\end{align*}

Further, we know if $p_2^* \in \bar B$, then $p^* = p_2^*$ and we are done.

Otherwise, we can just try again using the first constraint to find $p_1^*$. If $p_1^* \in \bar B$, then $p^* = p_1^*$ and we are done.

What if we get unlucky both times and $p_1^*, p_2^* \notin \bar B$?

\begin{tbox}{\textbf{Claim:} Because of convexity, if $p_1^*, p_2^* \notin \bar B$, then $p^* \in \Omega$}
    \emph{Proof:}

    \begin{center}
        \begin{tikzpicture}
            \fill[opacity=0.3] plot coordinates {
                    (0, 0)
                    (1, 2)
                    (2.4, 0)
                };

            \draw[name path=A] (4, 0) -- (0, 0)
            node[pos=0.1, red] {$\times$}
            node[pos=0.1, red, above] {$p_2$}
            node[left] {$\Omega_2$};

            \draw[name path=B] (3, -1) -- (1, 2)
            node[pos=0.1, red] {$\times$}
            node[pos=0.1, red, right] {$p_1$}
            node[above] {$\partial B$};

            \node at (1, 1) {$B$};

            \fill [blue, name intersections={of=A and B, by=x}] (x) circle (2pt) node[above right] {$p^*$};
        \end{tikzpicture}
    \end{center}


    WLOG, $p^* \in \Omega_1$ so let $\tilde p = [p^*, p_1^*] \cap \Omega \implies \tilde p \in \Omega$.
    \begin{center}
        \begin{tikzpicture}
            \draw[name path=A] (4, 0) -- (0, 0)
            node[pos=0, right] {$\{q: \E_q[\Ec_2(X)] = \theta_2\}$};

            \draw[name path=B] (3, -1) -- (1, 2)
            node[pos=0.7, red] {$\times$}
            node[pos=0.7, red, above right] {$p^*$}
            node[pos=0.1, red] {$\times$}
            node[pos=0.1, red, above right] {$p_1$};

            \fill [blue, name intersections={of=A and B, by=x}] (x) circle (2pt) node[above right] {$\tilde p$};

        \end{tikzpicture}
    \end{center}

    Then the $\tilde p$ should have been $p^*$ (contradiction.)

    Or
    \[\tilde p = \lambda p^* + (1- \lambda)p_{\perp}^* \quad \lambda (0, 1)\]
    so
    \[D(\tilde p \parallel p) \leq \lambda D(p^* \parallel p) + (1-\lambda) D(p_{\perp}^* \parallel p)\]
    but $D(p^* \parallel p)$ and $D(p_{\perp}^* \parallel p)$ are the smallest among the points while $D(\tilde p \parallel p)$ should be the largest. Contradiction.
\end{tbox}

\subsection{Information Point of View for Shannon Entropy}
In the following section, let $\log = \log_2$

Here, \tbf{Shannon Entropy} ``measures the minimal number of bits needed to encode a message optimally''.

For example, let $X_1, \dots, X_n \sim \{1, 2\}$ with $p= (p_1, p_2)$ and $p_2 = 1-p_1$.

As before, let $\hat p_1 = \frac{\#\{i: X_i = 1\}}{n}$ and $\hat p_2 = 1 - \hat p_1$.

\tbf{Question:} What is the probability of any particular sequence? (say $\hat p_1 \approx p_1, \hat p_2 \approx p_2$)

\emph{Answer:}
\begin{align*}
    \P(X_1= x_1, \dots, X_n = x_n) & = p_1^{\hat p_1 n} p_2^{\hat p_2 n}             \\
                                   & \approx p_1^{p_1 n} p_2^{p_2 n}                 \\
                                   & = 2^{n(\log p_1) p_1} \cdot 2^{n(\log p_2) p_2} \\
                                   & = 2^{-nH(p)}
\end{align*}
and this makes some sense: if we have no information, we would expect the probability of any sequence to be $2^{-n}$.

\section{Feb 12}

Let $\{X_i\}_{i=1}^n \sim \{0, 1\}$ with $p = (p_0, p_1) = (p_0, 1 - p_0)$. The Shannon Entropy is
\begin{align*}
    H(p) & = -\sum p_x \log p_x                                \\
         & = -p_0 \log p_0 - p_1 \log p_1                      \\
         & = -p_0 \log p_0 - (1 - p_0) \log (1 - p_0) = F(p_0)
\end{align*}
for some function $F$.

\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
                axis lines = middle,
                xlabel = $p_0$,
                ylabel = $H(p)$,
                xtick={1/2, 1},
                ytick={1},
                xmin = 0, xmax = 1.5,
                ymin = 0, ymax = 1.5,
                clip=false,
                axis equal
            ]
            \addplot[blue, domain=0:1.5] {-x * log2(x) - (1 - x) * log2(1 - x)};
            \node at (axis cs: 0.5, 1) {$\bullet$};
        \end{axis}
    \end{tikzpicture}
\end{center}

What is the relationship between the Shannon Entropy and the KL-Divergence?

\begin{align*}
    D(p \parallel h) & = \sum p_x \log \frac{p_x}{h_x}         \\
                     & = \sum p_x \log p_x - \sum p_x \log h_x \\
                     & = -H(p) - \log \frac{1}{s}
\end{align*}
for $h \sim \text{Unif}(1, s)$. Hence, up to a constant, $H(p) \approx D(p \parallel \text{Unif}\{1, \dots, s\})$.

And indeed this justifies that $H(p)$ has its max at $1/2$ when $p = (1/2, 1/2)$.

This also explains what we found last class: we only need $2^{nH(p)}$ bits rather than $2^n$ because in the worst case, $H(p) = 1 \implies 2^{n\cdot 1} = 2^n$.

\subsection{Source Coding}
More generally, we can take $X = (X_1, \dots, X_n) \sim p$ on states $\{1, \dots, t\}$ for $t = 2^n$.

Let $C: \{1, \dots, t\} \to \{0, 1\}^*$ be a \tbf{source code} where $\{0, 1\}^*$ is the set of finite non-empty strings of 0s and 1s.

We let $\abs{C(x)}$ denote the length of the code. In general, we want $\abs{C(x)}$ to be small across different $x$.

\tbf{Example:} A trivial code is the identity: $C(x) = x$ for all $x$. For $p = 1/2$, this is the best we can do.

If, however, $p = (0.99, 0.01)$ we can do better in expectation.

\tbf{Prefix:} A \emph{prefix code} is a code $C$ for which $C(x)$ is not a prefix for $C(\tilde x)$ for any $x \neq \tilde x$.

\emph{Example:}

\qquad \begin{tabular}{c|cc}
    $x$ & $C(x)$ & $C'(x)$ \\ \hline
    1   & 0      & 0       \\
    2   & 1      & 10      \\
    3   & 00     & 11      \\ \hline
\end{tabular}

Here, $C$ is not a prefix because under $C$, if we are trying to encode 0100, we do not know if it should be 120 or 1211. However, $C'$ is a prefix because there is no ambiguity.

\tbf{Remark:} Being a prefix is not necessary for unique decoding. For example,

\qquad \begin{tabular}{c|c}
    $x$ & $C(x)$ \\ \hline
    1   & 0      \\
    2   & 01     \\
    3   & 011    \\ \hline
\end{tabular}

is not a prefix but any string can be uniquely decoded by looking back.

\tbf{Question:} What is the minimal $(\abs{C(x)})_x$ (i.e. $C = \argmin \E_p \abs{C(x)} = \sum p_x \abs{C_x}$) where $C$ is a prefix code?

If we simply return the message, every encoded message is of equal length so $C$ is a prefix code of expected length $n$. Can we do better?

\begin{proposition}
    \textbf{Proposition (Kraft-McMillan Inequality):} For all prefix codes $C$,
    \[\sum_{x=1}^t 2^{-\abs{C(x)}} \leq 1\]
    and for any code lengths $\ell_1, \dots, \ell_t$ such that
    \[\sum_{x=1}^t 2^{-\ell_x} \leq 1\]
    there exists a a prefix code $C$ with $\abs{C_x} = \ell_x$ (letting $C_x = C(x)$).
\end{proposition}

\emph{Example:} In the non-prefix example, we say $\ell_1 = 1, \ell_2 = 2, \ell_3 = 3$ so
\[\sum_{x=1}^t 2^{-\ell_x} = 2^{-1} + 2^{-2} + 2^{-3} \leq 1\quad \checkmark \]

We can visualize this as a tree:
\begin{center}
    \begin{tikzpicture}
        \Tree [.{}
                [.0 \node{00}; \node{01};]
                [.1
                        [.{10} \node{100}; \node{101};]
                        [.{11} \node{110}; \node{111};]]
        ]
    \end{tikzpicture}
\end{center}


We will see next time that the optimal code $C^*$ satisfies $H(p) \leq \E\abs{C^*(X)} \leq H(p)$

\section{Feb 14}
\tbf{Motivation:} Let $p = (p_1, p_2)$ be a distribution on $\{0, 1\}$ ($s = 2$).

Sample $(X_1, \dots, X_n)$ corresponding to $n$ bits. Hence, there are $2^n$ possible sequences.

We can design a prefix code $C: \{0, 1\}^n \to \{0, 1\}^*$.

\emph{Example:} For $n = 3$,

\qquad\begin{tabular}{c|c}
    $X_1X_2X_3$ & $C(X_1X_2X_3)$ \\ \hline
    000         & 00             \\
    001         & 01             \\
    \vdots                       \\
    111
\end{tabular}

with $\E_p[\abs{C_x}] \approx H(p)n$. And indeed this is a prefix since every image is the same length.

We know that for the identity code, $C(x) = x$, $\E_p [\abs{C_{(X_1, \dots, X_n)}}] = n$.

\begin{proposition}
    \textbf{Theorem:} Let $\vec X \sim \vec p$. For the optimal code $C^* = \argmin_{C \text{ prefix}} \E_{\vec p} [\abs{C(X)}]$,
    \[H(\vec p) \leq \abs{\E_{\vec p}} C^*(X) \leq H(\vec p ) + 1\]
\end{proposition}

\tbf{Remark: }In our example, $\vec X = (X_1, \dots, X_n), \quad X_i \iid p$ so
\[H(\vec p) \leq \E_{\vec p} \abs{C(X)} \leq H(\vec p) + 1\]
where $\vec p = p \otimes \dots \otimes p$.

\begin{tbox}{\textbf{Claim:}
        \begin{enumerate}
            \item $H(\vec p) = nH(p)$.
            \item $H(X, Y) = H(X) + H(Y)$ if $X, Y$ independent
        \end{enumerate} }
    \emph{Proof:}
    1. Follows as a corollary from (2).

    \div

    2. Let $X$ take values $\{x_1, \dots, x_A\}$ and $Y$ take values $\{y_1, \dots, y_B\}$.

    Then
    \begin{align*}
        H(X, Y) & = -\sum_{i=1}^{AB} p_{i} \log p_{i}                                                        \\
                & = -\sum_{x=1}^A \sum_{y=1}^B p_{xy} \log p_{xy}                                            \\
                & = -\sum_x \sum_y p_x q_y \log p_x q_y \qquad (X, Y \text{ independent})                    \\
                & = -\sum_x \sum_y p_x q_y \log p_x + p_x q_y \log q_y                                       \\
                & = -\sum_y p_y \sum_x p_x \log p_x - \sum_x p_x \sum_y q_y \log q_y \qquad (\text{Tonelli}) \\
                & = \sum_y q_y H(x) + \sum_x p_x H(y)                                                        \\
                & = H(X) + H(Y) \qed
    \end{align*}
\end{tbox}

Hence,
\[nH(p)\leq \E{\abs{C(X)}} \leq nH(p) + 1\]

In particular, our propositions from earlier in the week follow immediately. Most importantly, we have confirmed that we indeed only need $2^{nH(p)}$ bits to encode a message.

At last, we are ready to actually prove the theorem:

\begin{tbox}{\textbf{Theorem:} Let $\vec X \sim \vec p$. For the optimal code $C^* = \argmin_{C \text{ prefix}} \E_{\vec p} [\abs{C(X)}]$,
        \[H(\vec p) \leq \abs{\E_{\vec p}} C^*(X) \leq H(\vec p ) + 1\]}
    \emph{Proof:} Let $X \sim p$.

    \begin{enumerate}
        \item $H(p) \leq \E_p \abs{C(X)}$

              Let $\ell_x = \abs{C_x}$. Then
              \begin{align*}
                  \E\abs{C(X)} - H(p) & = \sum p_x \ell_x + \sum p_x \log p_x                                                       \\
                                      & =  \sum p_x \log(2^{\ell_x} p_x)                                                            \\
                                      & = \sum p_x \log \frac{p_x}{2^{-\ell_x}}                                                     \\
                                      & = \sum p_x \log \frac{p_x}{2^{-\ell_x} \cdot \frac{\sum_y 2^{-\ell_y}}{\sum_y 2^{-\ell_y}}} \\
              \end{align*}

              Let $S = \sum_x 2^{-\ell_x}$. By Kraft-McMillan, $S \leq 1$ so

              \begin{align}
                   & = \sum_x p_x \log \frac{p_x}{q_x S}                   \\
                   & = \sum_x p_x \log \frac{p_x}{q_x} - \sum_x p_x \log S \\
                   & = D(p \parallel q) - \log S \geq 0
              \end{align}

        \item $\E\abs{C^*(X)} \leq H(p) + 1$.

              It suffices to show $\exists C$ prefix such that
              \[\E_p\abs{C(X)} \leq H(p) + 1\]

              In fact, our Part I gives us a place to start: We would like to find $\ell_x$ such that $q_x \propto 2^{-\ell_x} \approx p_x$. Hence, let $\ell_x = \left\lceil \log_2 \frac{1}{p_x} \right\rceil$.

              Now, we just need to show $\exists C$ prefix such that $\ell_x = \abs{C_x}$. But by Kraft-Mcmillan, it suffices to show $\sum_x 2^{-\ell_x} \leq 1$.

              With a little more work, we can show this exactly. Heuristically, if we did not need to round to get an integer $\ell_x$, we would have $H(p)$ exactly. Rounding, we get $H(p) + 1$.
    \end{enumerate}
\end{tbox}

\section{Feb 19}
\tbf{Example:} $s = 3$ with $p = (1/2, 1/4, 1/4)$.

Then
\[H(p) = \sum p_x \log \frac{1}{p_x} = \frac{1}{2} \log 2 + \frac{1}{4} \log 4 + \frac{1}{4} \log 4 = \frac{3}{2}\]

If we want to encode $X_1\cdots X_n$, we have $3^n$ possible sequences. We would naturally like to design a prefix code $C$ with length $\ceil{\log_2 \frac{1}{p_x}}$.

One way is via block coding. We first choose the lengths:

\qquad \begin{tabular}{ccc}
    $X_1$ & $p_x$ & $\ell_x = \ceil{\log_2 \frac{1}{p_x}}$ \\\hline
    $1$   & $1/2$ & $1$                                    \\
    $2$   & $1/4$ & $2$                                    \\
    $3$   & $1/4$ & $2$
\end{tabular}

If we say $C(1) = 0$, then we can prune the resulting tree for all other encodings:
\begin{center}
    \begin{tikzpicture}
        \Tree [.{}
                [.0 \node[red]{00}; \node[red]{01};]
                [.1 \node{10}; \node{11};]
        ]
    \end{tikzpicture}
\end{center}
which naturally leads us to a full prefix code:

\qquad \begin{tabular}{ccc}
    $X_1$ & $C(x)$ \\\hline
    1     & 0      \\
    2     & 10     \\
    3     & 11
\end{tabular}

\tbf{Example:} Now consider $s = 3$, $p = (1/3, 1/3, 1/3)$. Then $H(p) = \log 3 \approx 1.58$. So

For $n = 1$,

\qquad \begin{tabular}{cccc}
    $x$ & $p(x)$ & $\ell_x$               & $C(x)$ \\\hline
    $1$ & $1/3$  & $\ceil{\log_2(3)} = 2$ & 0      \\
    $2$ & $1/3$  & $2$                    & 10     \\
    $3$ & $1/3$  & $2$                    & 11
\end{tabular}

with
\[\E\abs{C_x} = \frac{2}{3}(2) + \frac{1}{3}(1) = \frac{5}{3}\]

But with $n=2$, we have $3^2 = 9$ possible sequences. Looking at the tree, we can choose a reasonable minimal encoding:

\begin{center}
    \begin{tikzpicture}
        \Tree [.{}
                [.{1}
                        [.{3} [.\node[blue]{7}; \node{15}; \node{16};]
                                [.\node[blue]{8}; \node{17}; \node{18};]]
                        [.{4} [.\node[blue]{9}; \node{19}; \node{20};]
                                [.\node[blue]{10}; \node{21}; \node{22};]]]
                [.{2}
                        [.{5} [.\node[blue]{11}; \node{23}; \node{24};]
                                [.\node[blue]{12}; \node{25}; \node{26};]]
                        [.{6} [.\node[blue]{13}; \node{27}; \node{28};]
                                [.{14} \node[blue]{29}; \node[blue]{30};]]]]
    \end{tikzpicture}
\end{center}

which gives
\qquad \begin{tabular}{cccc}
    $x$ & $p(x)$ & $\ell_x$ & $C(x)$ \\\hline
    11  & $1/3$  & 4        & 000    \\
    12  &        &          & 001    \\
    13  &        &          & \vdots \\
    21                               \\
    22                               \\
    23                               \\
    31  &        &          & 110    \\
    32  &        &          & 1110   \\
    33  &        &          & 1111
\end{tabular}

which has
\[\E\abs{C_x} = \frac{7}{9}(3) + \frac{2}{9}(4) \approx 3.222 = 1.611 \cdot 2\]
which means we use 1.611 bits per signal.

If $n \to \infty$, then the best prefix code has an average $H(p)$ bits per symbol.

\chapter{Statistical Inference}
\section{Feb 19}
\subsection{Probability Estimation}
\tbf{Motivation:} Let $X_1, X_2, \, \dots,\, X_n \iid P_{\theta}$. We want to estimate $\theta$.

\emph{Example:} If $X_1, \dots, X_n \sim \mathcal{N}(\mu, \sigma^2)$, then $\theta = (\mu, \sigma)$.

\tbf{Unbiased Estimation:} Suppose $\hat \theta = \hat \theta(x_1, \dots, x_n)$ is an estimation of $\theta$. If $\E[\hat \theta] = \theta$, we say $\hat \theta$ is \emph{unbiased.}

\tbf{Example:} Let $X_1, \dots, X_n \sim \Nc(\mu, \sigma^2)$.
\begin{itemize}
    \item $\hat \mu = \frac{1}{n}(X_1 + \dots + X_n)$ is unbiased since
          \[\E[\hat \mu] = \frac{1}{n} \sum \E[X_i] = \frac{1}{n}(n)(\mu) = \mu\]
    \item What is an unbiased estimator for $\sigma^2$? We know $\sigma^2 = \E[X^2] - (\E X)^2 = \E[(X - \mu)^2]$ so
          \[\hat \sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \hat \mu)^2 \]

    \item In fact, $\hat{\hat{\sigma^2}} = \frac{1}{n} \sum_{i=1}^n (X_i - \hat \mu)^2$ is a biased estimator:

          \begin{proof}
              \emph{Proof:} WLOG $\mu = 0$ (else $Y_i = X_i - \mu \sim \Nc(0, \sigma^2) \implies \hat \mu_X = \hat \mu_Y - \mu$).

              Then $\sigma^2 = \E[X^2]$ so
              \begin{align*}
                  \hat \mu          & = \frac{1}{n} \sum X_i                                                                                               \\
                  \hat \sigma^2     & = \frac{1}{n-1} \sum (X_i - \hat \mu)^2
                  \E[\hat \sigma^2] & = \E\left[\frac{1}{n-1} \sum (X_i - \hat \mu)^2\right]                                                               \\
                                    & = \frac{1}{n-1} \sum \E[(X_i - \hat \mu)^2]                                                                          \\
                                    & = \frac{n}{n-1} \E[(X_i - \hat \mu)^2]                                                                               \\
                                    & = \frac{n}{n-1} \E\left[\left(X_i - \frac{X_1 + \dots + X_n}{n}\right)^2\right]                                      \\
                                    & = \E\left[\left(\frac{n-1}{n}X_1 - \frac{1}{n} X_2 \cdots - \frac{1}{n} X_n\right)^2\right]                          \\
                                    & = \E\left[\left(\frac{n-1}{n}\right)^2 X_1^2 + \sum_{i=2}^{n} \frac{1}{n^2} X_1^2 + 2 \sum_{i\neq j} X_i X_j \right] \\
                                    & = (\frac{n-1}{n})^2 \E[X_1^2] + \frac{n-1}{n^2} \E[X_1^2]                                                            \\
                                    & = \frac{(n-1)^2}{n^2} \sigma^2                                                                                       \\
                                    & = \frac{n-1}{n} \sigma^2
              \end{align*}
              since for $i \neq j$, $\E[X_i X_j] \overset{X_i \perp X_j}{=} (\E X_i)(\E X_j)$
          \end{proof}
\end{itemize}

\tbf{Consistent:} We say $\hat \theta_n$ is \emph{consistent} if $\hat \theta_n \longrightarrow \theta$ in some sense as $n \to\infty$. For example,
\begin{itemize}
    \item $\hat \theta_n \overset{a.s.}{\longrightarrow} \theta \implies \P(\lim_{n \to \infty} \hat \theta_n = \theta) = 1$
    \item $\hat \theta_n \overset{P}{\longrightarrow} \theta \implies \forall \ep > 0, \P(\abs{\hat \theta_n - \theta} > \ep) \overset{n\to\infty}{\longrightarrow} 0$
    \item $\hat \theta \overset{\text{mean square}}{\longrightarrow} \theta \implies \E[(\hat \theta_n - \theta)^2] \to 0$.
\end{itemize}

Is $\hat \sigma^2$ consistent in any sense? As we will see, yes. But not trivially so.

\section{Feb 21}

\tbf{Recall:} Let $\theta = \sigma^2$ and take $X_1, X_2, \, \dots,\, X_n \iid p$. Then
\[S_n^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar X)^2\]
is an unbiased estimator for $\sigma^2$.

Further,
\[\hat \sigma_n^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \bar X)^2 \]
is a biased estimator for $\sigma^2$.

\tbf{Mean Squared Error (MSE):} $\MSE(\hat \theta_n) = \E \abs{\hat \theta_n - \theta}^2$.

Notice,
\begin{align*}
    \MSE(\hat \theta) & = \E(\hat \theta_n - \theta)^2                                                                                           \\
                      & = \E(\underbrace{\hat \theta_n - \E \hat \theta_n}_{a} + \underbrace{\E \hat \theta_n + \E\hat \theta_n - \theta}_{b})^2 \\
                      & = \E (a + b^2)                                                                                                           \\
                      & = \E a^2 + 2b \underbrace{\E a}_{0} + \underbrace{b^2}_{\text{bias}^2}                                                   \\
                      & = \Var(\hat \theta) + \text{bias}^2
\end{align*}

\tbf{Example:} Calculate $\MSE(S_n^2)$ vs. $\MSE(\hat \sigma_n^2)$. For simplicity, assume $\mu = 0$,$\sigma^2 = 1$ and $\E_p X^4 = 3$.

\begin{align*}
    \MSE(S_n^2) & = \Var(S_n^2) + \text{bias}^2                         \\
                & = \Var(S_n^2) \qquad \text{since $S_n^2$ is unbiased} \\
                & = \E[(S_n^2 - \E S_n^2)^2]                            \\
                & = \E[(S_n^2 - \sigma^2)^2]                            \\
                & = \E[(S_n^2 - 1)^2]                                   \\
                & = \E[S_n^4] - 2\E[S_n^2] + 1                          \\
                & = \E[S_n^4] - 2 + 1                                   \\
                & = \E[S_n^4] - 1
\end{align*}

We know
\begin{align*}
    S_n^2 & = \frac{1}{n-1} \left(\sum (X_i - \frac{\sum X_j}{n})\right)^2                                 \\
          & = \frac{1}{n-1} \sum_{i=1}^n \left(\frac{n-1}{n} X_i - \frac{1}{n} \sum_{j\neq i} X_j\right)^2
\end{align*}

We want
\begin{align*}
    \E[S_n^4] & = \frac{1}{(n-1)^2} \E\left[\sum_i \left(\frac{n-1}{n} X_i - \frac{1}{n} \sum_{j\neq i} X_j\right)^2\right]^2
\end{align*}
up to coefficients, we will only have $X_i^4, X_i^3X_j, X_i^2X_j^2, X_i^2X_jX_k, X_iX_jX_kX_l$ terms in the expansion.

Under expectation, however, only the $X_i^4$ and $X_i^2 X_j^2$ terms will survive.

After a little more work, we find
\begin{align*}
    \MSE(S_n^2)           & = \frac{2}{n -1} \sigma^4  \\
    \MSE(\hat \sigma_n^2) & = \frac{2n-1}{n^2}\sigma^4
\end{align*}
but then $\MSE(\hat \sigma_n^2) < \MSE(S_n^2)$ so even though it is biased, it is a better estimator (in the sense of minimizing MSE).

\subsection{Nonparametric Estimation}
\tbf{Example:} Let $X_1, X_2, \, \dots,\, X_n \iid p$. We want to estimate $p$.

Suppose we have one observation $\hat p_x = \frac{1}{n} \#\{i: X_i = x\}$. How good an estimator is this?

First, is it unbiased? We know that for a set $B$,
\[\hat p(B) = \frac{1}{n}\cdot \#\{i: X_i \in B\} = \sum_{x \in B} \hat p_x\]
and
\[\E[\hat p(B)] = \frac{1}{n} \sum_i \E[\ind_{X_i \in B}] = \frac{1}{n} \sum_i p(B) = p(B)\]
so $\hat p_x$ is unbiased.

Next, is it consistent? That is, for $B$ measurable, does $\hat p_n(B) \to p(B)$ in some sense?

By LLN,
\[\hat p_n(B) = \frac{1}{n} \sum_i \ind_{X_i \in B} = \frac{1}{n} \sum_i Y_i \overset{a.s.}{\longrightarrow}\E Y = \E \ind_{X_i \in B} = \P(X_i \in B) = p(B)\]

\begin{exercise}
    \textbf{Exercise:} In the above proof, we depended on $B$ being fixed. Here we show that this condition was necessary.

    Let $p = \Nc(0, 1)$. For all $n$, show that there exists a set $B_n(X_1, \dots, X_n)$ such that $\hat p_n(B_n)$ is far from $p(B_n)$.
\end{exercise}

\section{Feb 24}
\tbf{Motivation:} Let $f$ be the density of $p$. We want to estimate $f$. We can approximate $\hat p$ but this is discrete so we cannot have a continuous $\hat f$.

Formally, how can we approximate the Dirac measure $\delta_a(A) = \ind_{a \in A}$ by a continuous measure?

\subsection{Kernel Density Estimation}

\tbf{Density Function:} a function $k$ satisfying
\begin{enumerate}
    \item $k(x) \geq 0$
    \item $\int xk(x)\; dx = 0$
    \item $\int x^2 k(x)\; dx = 1$
\end{enumerate}
i.e $Y \sim k \implies \E Y = 0 \land \Var Y =1$.

\emph{Example:} $k(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$

\emph{Example:} We want to approximate $\delta_0$. For $Z = 0$, we know $\delta_0 = \text{dist}(Z)$.

One approach is to approximate $Z$ by $Z + Y$ where $Y$ is continuous (hence $\E Y =0$) and therefore $Z + Y$ is continuous.

A natural solution is $Y_{\ep} \sim \Nc(0, \ep)$ for $\ep \ll 1$. Notice, $Y_0 \sim \Nc(0, 1) \implies \ep Y_0 \sim \Nc(0, \ep^2)$.

In general, if $Y \sim k$, what is the density of $\ep Y$?

We can consider the CDF:
\begin{align*}
    F_Y(x)       & = \P(Y \leq x) = int_{-\infty}^{x} k(t)\; dt                                                \\
    F_{\ep Y}(x) & = \P(\ep Y \leq x) = \P\left(Y \leq \frac{x}{\ep}\right) = \int_{-\infty}^{x/\ep} k(s)\; ds \\
                 & \overset{s=t/\ep}{=} \int_{-\infty}^x k\left(\frac{t}{\ep}\right) \frac{dt}{\ep}            \\
                 & \implies k_{\ep}(t) = \frac{1}{\ep} k\left(\frac{t}{\ep}\right)
\end{align*}

\tbf{Definition:} for each \tbf{smoothing parameter} $w$ (aka bandwidth),
\[k_w(x) = \frac{1}{w} k\left(\frac{x}{w}\right) \]

\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
                width=0.3\textwidth,
                axis lines=middle,
                no markers,
                xtick={-2, -0.5, 0, 0.5, 2},
                ytick=\empty,
                xticklabels={$-1$, $-w$, {}, $w$, $1$},
                clip=false,
                xlabel={},
                ylabel={},
                domain=-3:3,
                ymin=0, ymax=2,
                xmin= -3, xmax=3,
                view = {0}{90}
            ]
            \addplot[blue, thick] {exp(-x^2)} node[right] {$Y \sim k$};
            \addplot[red, thick, samples=40] {1.5*exp(-4*x^2)} node[pos=0.6, right] {$wY \sim k_w$};

        \end{axis}
    \end{tikzpicture}
\end{center}

Now, our goal is to find the optimal $w$ to approximate $Z(\sim \delta_0)$ by $Z + Y_w$.

Correspondingly, we approximate $f(x)$ by
\[\hat f(x) = \hat f_{n, w}(x, X_1, \dots, X_n) = \frac{1}{n} \sum_{i=1}^n k_w(x - X_i)\]

\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
                width=0.3\textwidth,
                axis lines=middle,
                no markers,
                xtick={1, 3, 5},
                ytick=\empty,
                xticklabels={$x_1$, $x_2$, $x_3$},
                clip=false,
                xlabel={$x$},
                ylabel={$y$},
                domain=0:8,
                ymin=0, ymax=2,
                xmin=0, xmax=8,
                samples=50,
                view = {0}{90}
            ]
            \addplot[blue, thick] {exp(-2*(x-1)^2)};
            \addplot[red, thick] {exp(-2*(x-3)^2)};
            \addplot[green, thick] {exp(-2*(x-5)^2)};
        \end{axis}
    \end{tikzpicture}
\end{center}

Our plan is to use $\MSE = \text{bias}^2 + \text{variance}$ as
\begin{center}
    \begin{tabular}{c|cc}
        $w\searrow 0$      & $\text{bias} \searrow$ & $\text{variance} \nearrow$ \\
        $w\nearrow \infty$ & $\text{bias} \nearrow$ & $\text{variance} \searrow$
    \end{tabular}
\end{center}

\tbf{Integrated Square Error (ISE):}
\[\ISE = \int_{\R} \abs{\hat f_n(x, X_1, \dots, X_n) - f(x)}^2\; dx\]

Since this is a random variable, we can also define \emph{mean integrated square error}.

\tbf{Mean Integrated Square ERROR (MISE):}
\begin{align*}
    \text{MISE} = \E[\ISE] & = \int_{\R} \E\abs{\hat f(x, X_1, \dots, X_n) - f(x)}^2\; dx                                                                                                     \\
                           & = \int_{\R} \E\abs{\hat f(x, X_{1:n}) - \E[\hat f_n(x, X_{1:n})] + \E[\hat f_n(x, X_{1:n})]- f(x)}^2\; dx                                                        \\
                           & =\int_{\R} \abs{\E \hat f_n(x, X_{1:n}) - f(x)}^2\; dx + \int_{\R} \E\abs{\hat f_n(x, X_{1:n}) - \E[\hat f_n(x, X_{1:n})]}^2\; dx                                \\
                           & = \int_{\R} \underbrace{\abs{\E \hat f_n(x, X_{1:n}) - f(x)}^2}_{\text{bias}^2}\; dx + \int_{\R} \underbrace{\Var[\hat f_n(x, X_{1:n})]}_{\text{variation}}\; dx
\end{align*}

We can apply this formula to the kernel density estimator so we have bias:
\begin{align*}
    B_{n, w}(x) & = \E[\hat f_n(x, X_1, \dots, X_n)] - f(x)                         \\
                & = \frac{1}{n} \sum_{i=1}^n \E[k_w(x - X_i)] - f(x)                \\
                & = \frac{1}{n} \sum_{i=1}^n \int_{\R} f(t) k_w(x - t) \; dt - f(x) \\
                & = \int_{\R} f(t) k_w(x - t) \; dt - f(x)
\end{align*}

\section{Feb 26}

\tbf{Recall:} For a continuous density $f$ with $X_1, \dots, X_n \iid f$, we would like to estimate $f$ but our normal method $\hat p_n = \frac{1}{n} \sum_{i=1}^n \delta_{X_i}$ is discrete, hence insufficient.

Hence, we introduce the \emph{Kernel Density Estimator}:
\[\hat f_{n, w}(x, X_1, \dots, X_n) = \frac{1}{n} \sum_{i=1}^n k_w(x - X_i)\]
where
\[k_w(t) = \frac{1}{w} k(\frac{t}{w}), \quad k \text{ some density}\]
is parameterized by the bandwidth $w$.

\tbf{Remark:} Above, we are using the Dirac Measure $\delta_x(A) = \begin{cases}
        1 & \text{if } x \in A \\
        0 & \text{otherwise}
    \end{cases}$ instead of the indicator function ($\ind: \R \to \R$) because we need a measure and not a function.

\tbf{Goal:} Find the ``optimal'' $w$.

We introduced the \emph{Integrated Square Error} (ISE), $\int_x \abs{\hat f(x) - f(x)}^2\; dx$ and the \emph{Mean Integrated Square Error} (MISE)
\[\text{MISE} = \E[\text{ISE}]= \int_x [(\text{bias}(x))^2 + \Var(x)]\; dx\]
where
\begin{align*}
    \text{bias(x)} & = \E[\hat f(x)] - f(x)                            \\
    \E[\hat f(x)]  & = \frac{1}{n} \sum_{i=1}^n \E_{X_i}[k_w(x - X_i)] \\
                   & = \E[k_w(x - X_1)] \qquad (X_i \iid f)            \\
                   & = \int_{\R} f(t) k_w(x - t)\; dt
\end{align*}

\tbf{Convolution:} Let $Z \sim f$ and $Y \sim g$ be independent. Then
\[Z + Y \sim (f \star g)(x) = \int_{\R} f(t)\; g(x - t)\; dt\]

Hence,
\[\E[\hat f(x)] = (f \star k_w)(x)\]
which means that $\E[\hat f]$ is the density of $Z + Y_w$ where $Z \perp Y_w$ and $Z \sim f$ and $Y_w \sim k_w$.

What does this tell us about the behavior?
\begin{itemize}
    \item For $Y \sim k$, $Y_w \sim wY$ so $\E[\hat f] \to f$ as $w \to 0$.
    \item As $w \to \infty$, our support becomes infinitely large so $\E[\hat f] \to 0$.
\end{itemize}

Hence,
\[(\text{bias}(x))^2 = (\E[\hat f(x)] - f(x))^2 = \begin{cases}
        0      & w\to 0      \\
        f^2(x) & w\to \infty
    \end{cases}\]

Now, let's calculate the variance term:

\begin{align*}
    \Var(x) & = \Var(\hat f(x))                                                                                              \\
            & = \Var\left(\frac{1}{n} \sum_{i=1}^n k_w(x - X_i)\right)                                                       \\
            & = \frac{1}{n^2} \sum \Var(k_w(x- X_i)) \qquad (\text{independence})                                            \\
            & = \frac{1}{n}\Var(k_w(x - X_i)) \qquad (\text{identically distributed})                                        \\
            & = \underbrace{\frac{1}{n}\E[(k_w(x- X_i))^2]}_{V^{(1)}} - \underbrace{\frac{1}{n}[\E[k_w(x-x_i)]]}_{V^{(2)}}^2
\end{align*}

From our previous work,
\[V^{(2)} = \frac{1}{n} \mathcal{I}^2 \to \begin{cases}
        \frac{1}{n} f^2(x) & w\to 0      \\
        0                  & w\to \infty
    \end{cases}\]
and
\begin{align*}
    V^{(1)} & = \frac{1}{n} \int f(g) k^2_w(x - t)\; dt                                           \\
            & = \frac{1}{n} \frac{1}{w}\int f(t) \frac{1}{w} k^2\left(\frac{x - t}{w}\right)\; dt \\
            & = \frac{1}{n} \frac{1}{w} \int f(ws + t) k^2(s)\; ds \qquad (s = \frac{x - t}{w})   \\
            & \to \begin{cases}
                      \infty & w\to 0      \\
                      0      & w\to \infty
                  \end{cases}
\end{align*}
since the constant $\frac{1}{w}$ term dominates the bounded $f, k$.

\section{Feb 28}

\begin{proposition}
    \textbf{Theorem:} Assume $f$ and $k$ smooth. Then as $w \to 0$,
    \[\text{MISE}_{n, w} = \underbrace{\alpha w^4}_{\text{bias}} + \underbrace{\frac{\beta}{nw}}_{\text{variance}} + \text{error}\]
\end{proposition}

How do we choose $w$? Ignoring $\alpha, \beta$, it makes sense we want to minimize MISE:
\[(w^4 + \frac{1}{nw})' = 4w^3 - \frac{1}{nw^2} = 0 \implies w^5 \propto \frac{1}{n}  \implies w \propto n^{-1/5} \]

This is \tbf{Sylverman's Rule of Thumb:} up to unknown bias and variance, choose $w = n^{-1/5}$.

However, assuming we do not know $\alpha, \beta$, this is not a very good estimate -- it does not even depend on the density $f$! Can we do better?

Recall the setup: $X_1, X_2, \, \dots,\, X_n \iid f$ with estimator
\[\hat f_{n, w}(x) = \frac{1}{n} \sum_{i=1}^n k_w(x - X_i)\]

We want to find $w$. Last time, we looked at the MISE. This time, consider only the ISE. Our goal is to minimize:
\begin{align*}
    \text{ISE} & = \int_x \abs{\hat f_{n, w}(x) - f(x)}^2\; dx                     \\
               & = \int \hat f_{n, w}(x) - 2\int \hat f \cdot f + \int f^2(x)\; dx
\end{align*}

Define
\begin{align*}
    I & = \int_x \hat f_{n, w}(x) \cdot f(x)\; dx                                                                               \\
      & = \E_{X_{n+1} \sim f}[\hat f_{n, w}(X_{n+1})] \qquad (X_{1:n} \iid f)                                                   \\
      & = \E[\hat f_{n, w}(X_{n+1}; X_1, \dots, X_n) ]                                                                          \\
      & \approx \E[\hat f_{n-1, w}(X_{n}; X_1, \dots, X_{n-1})]                                                                 \\
      & \approx \frac{1}{n} \sum_{i=1}^n \E[\hat f_{n- 1, w}(X_i; i^X)] \qquad (i^X = X_1, \dots, X_{i-1}, X_{i+1}, \dots, X_n) \\
      & = \frac{1}{n} \sum_{i=1}^n \hat f_{n-1, w}^{(i)}(X_i)
\end{align*}

We call this the \tbf{cross-validation} (leave-one-out) estimator.

Since the last term does not depend on $w$, it suffices to find
\[\argmin_w \hat J(w) = \int \hat f_{n, w}(x)^2 - 2 \frac{1}{n} \sum_{i=1}^n \hat f_{n-1, w}^{(i)}(X_i)\]

And this is exactly what we want since this minimization problem depends only on the kernel and not on the distribution $f$.

\begin{proposition}
    \textbf{Theorem (Stone 1984):}
    \[\P\left(\lim_{n \to \infty} \frac{\ISE(\hat f_{\hat w_n}, f)}{\inf_w \ISE(\hat f_{w, n}, f)} = 1\right) = 1 \]
    (i.e. almost surely)
\end{proposition}

However, this convergence could be very slow (especially for $X_i \sim f \in \R^d, \; d \gg 1$)

\tbf{Example:} For $f$ Gaussian in $\R^d$ with $f(0) = \left(\frac{1}{\sqrt{2\pi}}\right)^d$, to have
\[\abs{\hat f_{\hat w_n} - f(0)} \leq \frac{1}{10} f(0)\]

\begin{center}
    \begin{tabular}{c|c}
        $d$ & $n$    \\ \hline
        1   & 4      \\
        2   & 19     \\
        5   & 768    \\
        10  & 842000 \\
        $\vdots$
    \end{tabular}
\end{center}

which is very fast growth

\section{March 3}
\subsection{Maximum Likelihood Estimation}

\tbf{Setup:} Sample $X_1,\, \dots,\, X_n \sim p_{\theta}$ with $\theta$ unknown. We want to find $\theta$ that makes $X_1 =x_1, \dots, X_n = x_n$ most likely (i.e. the parameter that defines the distribution that best fits the observation)

\begin{align*}
    \hat \theta & = \argmax_{\tilde \theta} p_{\tilde \theta}(X_1 = x_1, \dots, X_n = x_n)                                                        \\
                & = \argmax_{\tilde \theta} p_{\tilde \theta}(x_1)\cdots p_{\tilde \theta}(x_n)                                                   \\
                & = \argmax_{\tilde \theta} \frac{1}{n} \sum_{i=1}^n \log p_{\tilde \theta}(x_i)                                                  \\
                & = \argmax_{\tilde \theta} \sum_{x=1}^s (\log p_{\tilde \theta}(x)) \hat p(x)                                                    \\
                & = \argmin_{\tilde \theta} -\sum_{x=1}^{s} \hat p(x) \log p_{\tilde \theta}(x)                                                   \\
                & = \argmin_{\tilde \theta} -\sum_{x=1}^{s} \hat p(x) \log \frac{\hat p(x)}{p_{\tilde \theta}(x)} - \sum \hat p(x) \log \hat p(x) \\
                & = \argmin_{\tilde \theta} D(\hat p \parallel p_{\tilde \theta}) + H(\hat p)                                                     \\
                & = \argmin_{\tilde \theta} D(\hat p \parallel p_{\tilde \theta})
\end{align*}
since the entropy term does not depend on $\theta$.

\tbf{Conclusion:} MLE is equivalent to finding the distribution that is closest in KL-divergence to the empirical distribution.

\tbf{Example:} $X_1, \dots, X_n \sim \Nc(\mu, 1)$. Equivalently, $f_{\mu} = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2}}$.

Hence,
\begin{align*}
    \hat \mu & = \argmax_{\tilde \mu} \prod_{i=1}^n f_{\mu}(x_i)               \\
             & = \argmax \exp\left(\sum -\frac{(x_i - \tilde \mu)^2}{2}\right) \\
             & = \argmin \sum_{i=1}^n (x_i - \tilde \mu)^2                     \\
             & \overset{*}{=} \frac{1}{n} \sum_{i=1}^n x_i
\end{align*}

\begin{exercise}
    \textbf{Exercise:} Prove the starred equality above.
\end{exercise}

\tbf{Example:} $X_1, \dots, X_n \sim f_{\lambda} = \frac{1}{Z_{\lambda}} p(x) \exp\left(\sum_{j=1}^c \lambda_j \Ec_j(x)\right) $

Then
\begin{align*}
    \hat \lambda & = \argmax \prod f_{\tilde \lambda} (x_i)                                                                                                                    \\
                 & = \argmin -\frac{1}{n} \sum_{i=1}^{n} \log(f_{\tilde \lambda}(x_i)) \qquad (\text{invariant under constant})                                                \\
                 & = \argmin -\frac{1}{n} \sum_{i=1}^{n} \log\left(\frac{1}{Z_{\lambda}} p(x_i) \exp\left(\sum_{j=1}^c \lambda_j \Ec_j(x)\right)\right)                        \\
                 & = \argmin -\frac{1}{n} \sum_{i=1}^{n} \log\left(\frac{1}{Z_{\lambda}} \exp\left(\sum_{j=1}^c \lambda_j \Ec_j(x)\right)\right) \qquad (p(x_i) \text{ known}) \\
                 & = \argmin \log(Z_{\tilde \lambda}) - \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^{c} \tilde \lambda_j \Ec_j(x_i)                                                    \\
                 & = \argmin \log(Z_{\tilde \lambda}) - \sum_{j=1}^{c} \lambda_j \left(\frac{1}{n} \sum_{i=1}^n \Ec_j(x_i)\right)                                              \\
                 & = \argmin \log(Z_{\tilde \lambda}) - \sum_{j=1}^{c} \lambda_j \theta_j
\end{align*}
where $\theta_j = \frac{1}{n} \sum_{i=1}^n \Ec_j(x_i)$ are the observed statistics.

\subsection{Classification}

\tbf{Motivation:} Suppose we set up outside the dining hall and observe the patterns of the rush. There is a large group that comes in at noon and another group that comes in later

\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
                width=0.3\textwidth,
                axis lines=middle,
                no markers,
                xtick={1.5, 3},
                xticklabels={12:00, 1:00},
                ytick=\empty,
                clip=false,
                xlabel={},
                ylabel={},
                domain=0:6,
                ymin=0, ymax=2,
                xmin=0, xmax=6,
                view = {0}{90}
            ]
            \addplot[blue] {1.5*e^(-2*(x-1.5)^2)};
            \addplot[red] {0.5*e^(-(x-3)^2)};
        \end{axis}
    \end{tikzpicture}
\end{center}

If we interviewed a student at 2:00, it is quite likely they will be from group two. Similarly, if we interviewed a student at 10:00, they are likely from group one. But what about at 12:30? This is the problem of classification.

Formally, let $X \in \R^d$ be some random variable. Let $Y = \{1, \dots, c\}$ be classes with $\pi_i = \P(Y = i)$ and $f_i(x)$ the class conditioned density (in the example above, $f_2$ would be the red curve).

Then for any set $A$,
\[P_X(A) = \sum_{i=1}^{c} \pi_i \P(A)\]
and
\[f_X(A) = \sum_{i=1}^{c} \pi_i f_i(A)\]

We define a \tbf{classification} $h: \R^d \to \{1, \dots, c\}$.

\section{March 5}

\begin{proposition}
    \textbf{Bayes' Classification Rule:}
    \[h^*(x) = \argmax_{i =1:c} \P(Y = i \; | \; X = x) \]
\end{proposition}

\tbf{Example:} $Y \in \{1, 2\}$.
\begin{align*}
    \P(Y = 1\; | \; X = x) & = \frac{\P(Y= 1, X = x)}{\P(X = x)}               \\
                           & = \frac{\P(Y=1) \P(X = x \; | \; Y= 1)}{\P(X= x)} \\
                           & = \frac{\pi_1 \cdot f_1(x)}{\P(X = x)}            \\
    \P(Y = 2\; | \; X = x) & = \frac{\pi_2 \cdot f_2(x)}{\P(X = x)}
\end{align*}

Hence,
\[h^*(x) = \begin{cases}
        2 & \pi_1 f_1(x) < \pi_2 f_2(x) \\
        1 & \text{otherwise}
    \end{cases}\]

In what sense can we say $h^*$ is the ``best'' classifier?
\[\P(h^*(X) \neq Y) \leq \P(h(x) \neq Y) \qquad \forall h: \R^d \to \{1, \dots, c\}\]

\begin{exercise}
    \textbf{Exercise:} Prove the optimality of Bayes' classification rule. Hint:
    \[\P(h^*(X) = Y) = \int_x \P(Y = h^*(x) \; | \; X = x) f_X(x)\; dx\]
\end{exercise}

\begin{proof}
    \emph{Proof:}
    \begin{align*}
        \P(h^*(x) \neq Y) & = 1 - \P(h^*(x) = Y)                                                             \\
                          & = 1 - \int_x \P(Y = h^*(x) \; | \; X = x) f_X(x)\; dx                            \\
                          & = 1 - \int_x \P(Y = \argmax_i [\P(Y = i \; | \; X = x)]; | \; X = x) f_X(x)\; dx \\
                          & \leq 1 - \int_X \P(Y = h(x) \; | \;X = x) f_X(x)\; dx                            \\
                          & = 1 - \P(h(X) = Y)                                                               \\
                          & = \P(h(X) \neq Y)
    \end{align*}
\end{proof}

In applications, however, we may be able to approximate the $f_i$'s by sampling but not necessarily the $\pi_i$'s.

\begin{proposition}
    \textbf{Neyman-Pearson (NP) Classification:} Fix $t \in (0, \infty)$. Then
    \[h_t(x) = \begin{cases}
            1 & \text{if } \frac{f_2(x)}{f_1(x)} < t \\
            2 & \text{if } \frac{f_2(x)}{f_1(x)} > t \\
        \end{cases}\]
\end{proposition}

\tbf{Remark:} In the case $t = \frac{\pi_1}{\pi_2}$, then NP is equivalent to Bayes.

If $Y = 1$ represents a negative test and $Y = 2$ represents a positive test, we have
\begin{itemize}
    \item The detection rate $\P(h(X) = 2 \; | \; Y = 2)$
    \item The false alarm rate $\P(h(X) = 2 \; | \; Y = 1)$
\end{itemize}

Intuitively, we would like to maximize the detection rate while minimizing the false alarm rate.

\begin{tbox}{\textbf{Theorem:} Fix $t \in (0, \infty)$. Let $h$ be any other classifier. If $\text{FAR}_h \leq \text{FAR}_{h_t}$, then $\text{DR}_h \leq \text{DR}_{h_t}$}
    \emph{Intuition:}

    We have
    \[\text{FAR}_{h_t} = \P(h(X) = 2 \; | \; Y = 1) = \P(\frac{f_2(x)}{f_1(x)} > t \; | \; Y= 1)\]
    so as $t \to \infty$, $\text{FAR}_{h_t} \searrow$ and $\text{DR}_t \searrow$

    Under NP classification,
    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[
                    width=0.3\textwidth,
                    axis lines=middle,
                    no markers,
                    xtick={1},
                    ytick={1},
                    clip=false,
                    xlabel={FAR},
                    ylabel={DR},
                    domain=-2:2,
                    ymin=0, ymax=1.5,
                    xmin=0, xmax=1.5,
                    view = {0}{90}
                ]
                \draw[blue] (0, 0) to[bend left] (1, 1) node[right] {$t = 0$};
            \end{axis}
        \end{tikzpicture}
    \end{center}

    \div

    \emph{Proof:}

    \begin{center}
        \begin{tikzpicture}

            \draw (-3, -3) rectangle (5, 3) node[below left] {$\Omega$};

            \draw (0, 0) circle (2);
            \draw (2, 0) circle (2);

            \node at (0, 1.5) {\tbf{$A$}};
            \node at (2, 1.5) {\tbf{$\tilde A$}};
            \node at (1, 1.2) {\tbf{$B$}};

            \node at (-1, 0.5) {$h = 2$};
            \node at (-1, -0.5) {$h_t = 1$};

            \node at (3, 0.5) {$h = 1$};
            \node at (3, -0.5) {$h_t = 2$};

            \node at (1, 0.5) {$h = 2$};
            \node at (1, -0.5) {$h_t = 2$};
        \end{tikzpicture}
    \end{center}

    We have
    \begin{align*}
        \text{FAR}_h & = \P(h(X) = 2 \; | \; Y=1)   \\
                     & = \P(A \cup B \; | \; Y = 1) \\
                     & = p_1(A \cup B)
    \end{align*}
    where $p_1$ is the marginal conditioned on the class being $1$.

    Then:
    \begin{enumerate}
        \item $p_1(A \cup B) \leq p_1(\tilde A \cup B) \implies p_1(A) \leq p_1(\tilde A)$ (since $A, B$ disjoint).

              We want to show $p_{\alpha}(A) \leq p_2(\tilde A)$

              Notice
              \begin{align*}
                  A        & \sub \{h_t(x) = 1\} = \left\{\frac{f_2(x)}{f_1(x)} < t\right\} = \{f_2(X) < t \cdot f_1(X)\} \\
                  \tilde A & \sub \{h_t(x) = 2\} = \left\{\frac{f_2(x)}{f_1(x)} > t\right\} = \{f_2(X) > t \cdot f_1(X)\} \\
              \end{align*}

              We have
              \begin{align*}
                  p_1(X \in A)                         & \leq p_1(X\in \tilde A)             \\
                  p_1(X \in A) = t \int_A f_1(X)\; d\P & \leq t \int_{\tilde A} f_1(X)\; d\P
              \end{align*}
    \end{enumerate}
\end{tbox}

\section{March 7}

\tbf{Motivation:} for training data $(X_1, Y_1) \dots (X_n, Y_n)$ with $X_i \in \R^d$ and $Y_i \in \{1, \dots, s\}$, we would like to build $h: \R^d \to \{1, \dots, s\}$.

There are multiple different approaces:
\begin{itemize}
    \item Generative
    \item Discriminative
    \item Algorithmic
\end{itemize}

\subsection{Generative Classifiers}
\[h^*(x) = \argmax_{x \in \{1, \dots, s\}} \P(Y = c \; | \; X = x) = \argmax_c \frac{\pi_c f_c(x)}{\P(X = x)}\]

A good estimator given data $(X_1, Y_1) \dots (X_n, Y_n)$ is clearly
\[\hat \pi_c = \frac{\#\{i: Y_i = c\}}{n}\]

But what if we do not know $f_c$? This gets especially difficult when $d$ is large.

\tbf{Naive Bayes:} Assume $X = (X^1, X^2, \dots, X^d)$ and $f_c(x^1, \dots, x^d) = f_c^1(x^1) \cdots f_c^d(x^d)$.

Then instead of needing to find $(f_c)^s$ with $f_c: \R^d \to \R$, it suffices to find $(f_c^k)_{k=1:d}^{c=1:s}$ for $f_c^k: \R \to \R$

\tbf{Quadratic Discriminant Analysis (QDA):}
Assume
\[f_c(x) \sim \Nc(\mu_c, \Sigma_c)\]

Then
\[f_c(x, \mu_c, \Sigma_c) = \frac{1}{\sqrt{(2\pi)^d \abs{\Sigma}}} \exp(-\frac{1}{2} (x - \mu_c)^T \Sigma_c^{-1}(x - \mu))\]
for $x \in \R^{d \times 1}$, $\mu_c \in \R^{d \times 1}$, $\Sigma_c \in \R^{d\times d}$

However, if we are to attempt MLE on $\mu_c, \Sigma_c$, we need to ensure that we have enough data $n$ to estimate the $d^2s$ parameters across classes $\{1, \dots, s\}$. In practice, this can lead to over fitting.

\section{March 10}

\tbf{Definition:} If we partition a space $\Omega = \bigcup_{i=1}^s A_i$ into disjoint sets, then \emph{precision boundary} of $A_i$ is $\partial A_i = A_i \setminus \overset{\circ}{A}_i$.

Last time, we saw a classification method that let us use the MLE on high-dimensional spaces but which required a lot of data in practice. This was the Quadratic Discriminant Analysis (QDA), which had a quadratic precision boundary.

\begin{center}
    \begin{tikzpicture}
        \draw (0, 0) rectangle (4, 4);
        \draw (3, 4) arc (0:-90:3);
        \draw (1, 1.2) arc (10:-90:1);
        \draw (2.2, 2) arc (90:0:1.8);
        \draw (4, 3.8) arc (90:270:1.2);




    \end{tikzpicture}
\end{center}

We can follow a similar but slightly less flexible approach.

\tbf{Linear Discriminant Analysis:}

Assume $f_c \sim \Nc(\mu_c, \Sigma)$.

Then the precision boundary is given by
\[\left\{x \in \R^d: \frac{f_2(x)}{f_1(t)} = t\right\}\]
from NP. We claim this is a linear set.

\begin{proof}
    \emph{Proof:} By assumption, $f_1 \sim \Nc(\mu_1, \Sigma)$ where $\mu_1 \in \R^d$ and $\Sigma \in \R^{d \times d}$.

    Then
    \[f_1(x) = \frac{1}{(2\pi)^{d/2} \abs{\Sigma}^{d/2}} \exp\left(-\frac{1}{2} (x- \mu_1)^T \Sigma^{-1}(x-\mu_1)\right) \]
    notice
    \[(x- \mu_1)^T \Sigma^{-1}(x-\mu) \in \R^{(1 \times d)(d \times d)(d \times 1)} = \R^1\]

    Similarly, we can write
    \[f_2(x) = \frac{1}{(2\pi)^{d/2} \abs{\Sigma}^{d/2}} \exp\left(-\frac{1}{2} (x- \mu_2)^T \Sigma^{-1}(x-\mu_2)\right) \]
    so
    \[\log t = \log \frac{f_1}{f_2} = \frac{1}{2} (x- \mu_1)^T \Sigma^{-1} (x - \mu_1) - (x - \mu_2)^T \Sigma^{-1} (x - \mu_2)\]

    In the case $d=1$, we have $\Sigma = \sigma^2$ so
    \[\log t = \frac{(x- \mu_1)^2}{\sigma^2} - \frac{(x - \mu_2)^2}{\sigma^2} = -\frac{2x(\mu_1 - \mu_2) + \mu_1^2 + \mu_2^2}{\sigma^2}\]
    but in the QDA case, we would have $\Sigma = (\sigma^2_1, \sigma^2_2)$ so the terms would not cancel.
\end{proof}

\subsection{Discriminative Construction}
Recall that in the Bayes' classification rule (the optimal case),
\[h^*(x) = \argmax_{c \in \{1, \dots, s\}} \P(Y = c \; | \; X = x)\]

Earlier, we wrote $\P(Y = c \; | \; X= x) = \pi_c f_c(x)$ and tried to estimate $\pi_c$ and $f_c$. But what if we tried to estimate $r_c(x) = \P(Y = c \; | \; X = x)$ directly?

\tbf{Linear Regression:} For $s= 2$, we want $r_1(x)$ and $r_2(x)$ satisfying $r_1(x) + r_2(x) = 1$. It seems reasonable to try linear regression.

We can model
\begin{gather*}
    \log \frac{r_2(x)}{r_1(x)} = \alpha + \beta x\\
    \log \frac{r_2}{1-r_2} = \alpha + \beta x\\
    r_2 = \frac{e^{\alpha + \beta x}}{1 + e^{\alpha + \beta x}}
\end{gather*}

\tbf{Softmax:} Softmax is a generalization of logistic regression for $s > 2$.

As before,
\[\log \frac{r_k(x)}{r_1(x)} = \alpha_k + \beta_k x \implies r_k(x) = \frac{e^{\alpha_k + \beta_k x}}{1 + \sum_{k=n}^s e^{\alpha_k + \beta_k x} }\]

Then we can use MLE to estimate $\alpha_k, \beta_k$.

\tbf{k-Nearest Neighbor Classification:}

Let $D_k(x)$ be the closed ball centered at $x$ with radius $R_k(x)$, the smallest radius that contains $k$ data points.

Then
\end{document}